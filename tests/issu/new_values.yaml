image:
  repository: memgraph/memgraph
  # It is a bad practice to set the image tag name to latest as it can trigger automatic upgrade of the charts
  # With some of the pullPolicy values. Please consider fixing the tag to a specific Memgraph version
  # tag: 3.5.0
  # tag: 3.5.0_3_5e192412b520-relwithdebinfo
  # tag: 3.5.0_5_56c7108feb03-relwithdebinfo
  tag: 3.5.0_23_4a76cb966ae5
  pullPolicy: IfNotPresent

env:
  MEMGRAPH_ENTERPRISE_LICENSE: ""
  MEMGRAPH_ORGANIZATION_NAME: ""

storage:
  data:
    libPVCSize: "1Gi"
    libStorageAccessMode: "ReadWriteOnce"
    # By default the name of the storage class isn't set which means that the default storage class will be used.
    # If you set any name, such storage class must exist.
    libStorageClassName: csi-hostpath-delayed
    logPVCSize: "1Gi"
    logStorageAccessMode: "ReadWriteOnce"
    logStorageClassName: csi-hostpath-delayed
    ## Create a Persistant Volume Claim for core dumps.
    createCoreDumpsClaim: false
    coreDumpsStorageClassName: 
    coreDumpsStorageSize: 10Gi
    coreDumpsMountPath: /var/core/memgraph
  coordinators:
    libPVCSize: "1Gi"
    libStorageAccessMode: "ReadWriteOnce"
    # By default the name of the storage class isn't set which means that the default storage class will be used.
    # If you set any name, such storage class must exist.
    libStorageClassName: csi-hostpath-delayed
    logPVCSize: "1Gi"
    logStorageAccessMode: "ReadWriteOnce"
    logStorageClassName: csi-hostpath-delayed
    ## Create a Persistant Volume Claim for core dumps.
    createCoreDumpsClaim: false
    coreDumpsStorageClassName:
    coreDumpsStorageSize: 10Gi
    coreDumpsMountPath: /var/core/memgraph

ports:
  boltPort: 7687   # If you change this value, change it also in probes definition
  managementPort: 10000
  replicationPort: 20000
  coordinatorPort: 12000  # If you change this value, change it also in probes definition

externalAccessConfig:
  dataInstance:
    # Empty = no external access service will be created
    serviceType: ""
    annotations: {}
  coordinator:
    # Empty = no external access service will be created
    serviceType: ""
    annotations: {}

headlessService:
  enabled: false  # If set to true, each data and coordinator instance will use headless service

# Affinity controls the scheduling of the memgraph-high-availability pods.
# By default data pods will avoid being scheduled on the same node as other data pods,
# and coordinator pods will avoid being scheduled on the same node as other coordinator pods.
# Deployment won't fail if there is no sufficient nodes.
affinity:
  # The unique affinity, will schedule the pods on different nodes in the cluster.
  # This means coordinators and data nodes will not be scheduled on the same node. If there are more pods than nodes, deployment will fail.
  unique: false
  # The parity affinity, will enable scheduling of the pods on the same node, but with the rule that one node can host pair made of coordinator and data node.
  # This means each node can have max two pods, one coordinator and one data node. If not sufficient nodes, deployment will fail.
  parity: false
  # The nodeSelection affinity, will enable scheduling of the pods on the nodes with specific labels. So the coordinators will be scheduled on the nodes with label coordinator-node and data nodes will be scheduled on the nodes with label data-node. If not sufficient nodes, deployment will fail.
  nodeSelection: false
  roleLabelKey: "role"
  dataNodeLabelValue: "data-node"
  coordinatorNodeLabelValue: "coordinator-node"

# If you are experiencing issues with the sysctlInitContainer, you can disable it here.
# This is made to increase the max_map_count, necessary for high memory loads in Memgraph
# If you are experiencing crashing pod with the: Max virtual memory areas vm.max_map_count is too low
# you can increase the maxMapCount value.
# You can see what's the proper value for this parameter by reading
# https://memgraph.com/docs/database-management/system-configuration#recommended-values-for-the-vmmax_map_count-parameter
sysctlInitContainer:
  enabled: true
  maxMapCount: 262144
  image:
    repository: library/busybox
    tag: latest
    pullPolicy: IfNotPresent

# This is fine because under both
# Memgraph and Mage images we actually hard-code the user and group id.
memgraphUserId: 101
memgraphGroupId: 103

secrets:
  enabled: false
  name: memgraph-secrets
  userKey: USER
  passwordKey: PASSWORD

labels:
  coordinators:
    podLabels: {}
    statefulSetLabels: {}
    serviceLabels: {}
  data:
    podLabels: {}
    statefulSetLabels: {}
    serviceLabels: {}

container:
  data:
    readinessProbe:
      tcpSocket:
        port: 7687  # If you change bolt port, change this also
      failureThreshold: 20
      timeoutSeconds: 10
      periodSeconds: 5
    livenessProbe:
      tcpSocket:
        port: 7687  # If you change bolt port, change this also
      failureThreshold: 20
      timeoutSeconds: 10
      periodSeconds: 5
    # When restoring Memgraph from a backup, it is important to give enough time app to start. Here, we set it to 2h by default.
    startupProbe:
      tcpSocket:
        port: 7687  # If you change bolt port, change this also
      failureThreshold: 1440
      timeoutSeconds: 10
      periodSeconds: 5
  coordinators:
    readinessProbe:
      tcpSocket:
        port: 12000  # If you change coordinator port, change this also
      failureThreshold: 20
      timeoutSeconds: 10
      periodSeconds: 5
    livenessProbe:
      tcpSocket:
        port: 12000  # If you change coordinator port, change this also
      failureThreshold: 20
      timeoutSeconds: 10
      periodSeconds: 5
    startupProbe:
      tcpSocket:
        port: 12000
      failureThreshold: 20
      timeoutSeconds: 10
      periodSeconds: 5

resources:
  data: {}
  coordinators: {}


updateStrategy:
  type: OnDelete  # Set to OnDelete to support ISSU

prometheus:
  enabled: false
  namespace: monitoring  # Namespace where K8s resources from mg-exporter.yaml will be installed and where your kube-prometheus-stack chart is installed
  memgraphExporter:
    port: 9115
    pullFrequencySeconds: 5
    repository: memgraph/prometheus-exporter
    tag: 0.2.1
  serviceMonitor:
    kubePrometheusStackReleaseName: kube-prometheus-stack
    interval: 15s

# If setting the --memory-limit flag under data instances, check that the amount of resources that a pod has been given is more than the actual memory limit you give to Memgraph
# Setting the Memgraph's memory limit to more than the available resources can trigger pod eviction and restarts before Memgraph can make a query exception and continue running
# the pod.
data:
- id: "0"
  restoreDataFromSnapshot: false
  volumeSnapshotName: data-0-snap  # Used only if restoreDataFromSnapshot is set to true
  args:
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--data-directory=/var/lib/memgraph/mg_data"

- id: "1"
  restoreDataFromSnapshot: false
  volumeSnapshotName: data-1-snap  # Used only if restoreDataFromSnapshot is set to true
  args:
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--data-directory=/var/lib/memgraph/mg_data"

coordinators:
- id: "1"
  restoreDataFromSnapshot: false
  volumeSnapshotName: coord-1-snap  # Used only if restoreDataFromSnapshot is set to true
  args:
  - "--coordinator-id=1"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-1.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--data-directory=/var/lib/memgraph/mg_data"

- id: "2"
  restoreDataFromSnapshot: false
  volumeSnapshotName: coord-2-snap  # Used only if restoreDataFromSnapshot is set to true
  args:
  - "--coordinator-id=2"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-2.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--data-directory=/var/lib/memgraph/mg_data"

- id: "3"
  restoreDataFromSnapshot: false
  volumeSnapshotName: coord-3-snap  # Used only if restoreDataFromSnapshot is set to true
  args:
  - "--coordinator-id=3"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-3.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--data-directory=/var/lib/memgraph/mg_data"
