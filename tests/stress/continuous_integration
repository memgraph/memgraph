#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Memgraph Stress Test Runner

Usage:
  # Run all enabled workloads for a deployment type
  ./continuous_integration --deployment docker_ha

  # Run a specific config file (legacy mode)
  ./continuous_integration --config-file docker_ha/workloads/rag/vector_workload.yaml
"""
import argparse
import multiprocessing
import os
import subprocess
import sys
import time
from argparse import Namespace as Args
from enum import Enum
from typing import Any, Dict, List, Optional

import yaml
from gqlalchemy import Memgraph
from workers import get_worker_object, get_worker_steps

# Paths
SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
BASE_DIR = os.path.normpath(os.path.join(SCRIPT_DIR, "..", ".."))
BUILD_DIR = os.path.join(BASE_DIR, "build")
MEASUREMENTS_FILE = os.path.join(SCRIPT_DIR, ".apollo_measurements")
KEY_FILE = os.path.join(SCRIPT_DIR, ".key.pem")
CERT_FILE = os.path.join(SCRIPT_DIR, ".cert.pem")
STATS_FILE = os.path.join(SCRIPT_DIR, ".long_running_stats")

# Get number of threads
THREADS = os.environ["THREADS"] if "THREADS" in os.environ else multiprocessing.cpu_count()


# =============================================================================
# Deployment
# =============================================================================


class DeploymentType(str, Enum):
    """Supported deployment types for stress testing."""

    NATIVE_STANDALONE = "native_standalone"
    DOCKER_HA = "docker_ha"
    EKS_HA = "eks_ha"
    NATIVE_HA = "native_ha"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def values(cls) -> List[str]:
        """Return list of all deployment type values."""
        return [d.value for d in cls]


def get_deployment_path(deployment: str) -> str:
    """Get the full path to a deployment folder."""
    return os.path.join(SCRIPT_DIR, deployment)


def get_workloads_registry(deployment: str) -> Optional[str]:
    """Get the path to workloads.yaml for a deployment."""
    registry_path = os.path.join(get_deployment_path(deployment), "workloads.yaml")
    if os.path.exists(registry_path):
        return registry_path
    return None


def load_workloads_registry(deployment: str) -> List[Dict[str, Any]]:
    """
    Load the workloads registry for a deployment.
    Returns list of enabled workload configs.
    """
    registry_path = get_workloads_registry(deployment)
    if not registry_path:
        print(f"Warning: No workloads.yaml found for deployment '{deployment}'")
        return []

    with open(registry_path) as f:
        registry = yaml.safe_load(f)

    workloads = registry.get("workloads", [])
    enabled_workloads = []

    for workload in workloads:
        config_path = workload.get("config")
        enabled = workload.get("enabled", True)

        if not enabled:
            print(f"  Skipping (disabled): {config_path}")
            continue

        # Resolve full path (relative to deployment folder)
        full_config_path = os.path.join(get_deployment_path(deployment), config_path)
        if not os.path.exists(full_config_path):
            print(f"  Warning: Config not found: {full_config_path}")
            continue

        enabled_workloads.append(
            {
                "config_path": full_config_path,
                "relative_path": config_path,
            }
        )

    return enabled_workloads


def start_deployment(script_path: str, args: List[str], env: Dict[str, str] = None) -> None:
    cmd = [script_path, "start"] + args
    print(f"Starting Memgraph with: {' '.join(cmd)}")

    # Merge provided env with current environment
    run_env = os.environ.copy()
    if env:
        for key, value in env.items():
            # Only set if value is non-empty, otherwise use existing env var
            if value:
                run_env[key] = str(value)

    subprocess.run(cmd, check=True, env=run_env)
    time.sleep(2)  # Ensure the server has time to start


def stop_deployment(script_path: str) -> None:
    cmd = [script_path, "stop"]
    print(f"Stopping Memgraph with: {' '.join(cmd)}")
    subprocess.run(cmd, check=True)


def check_deployment_status(script_path: str) -> bool:
    cmd = [script_path, "status"]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return result.returncode == 0


# =============================================================================
# Config Class
# =============================================================================


class Config:
    def __init__(self, config):
        self._verbose = config.get("general", {}).get("verbose", False)
        self._use_ssl = config.get("general", {}).get("use_ssl", False)

        # Memgraph configuration
        memgraph = config.get("memgraph", {})
        self._script = memgraph.get("deployment", {}).get("script", None)
        self._memgraph_args = memgraph.get("args", [])
        self._env = memgraph.get("env", {})

        # Cluster configuration (for HA deployments)
        cluster_config = config.get("cluster", {})
        self._coordinators = cluster_config.get("coordinators", [])
        self._data_instances = cluster_config.get("data_instances", [])

        # Dataset tests
        dataset = config.get("dataset", {})
        self._tests = dataset.get("tests", [])

        self._custom_workloads = config.get("customWorkloads", {}).get("tests", [])

    @property
    def uses_ssl(self):
        return self._use_ssl

    @property
    def is_verbose(self):
        return self._verbose

    @property
    def script(self):
        return self._script

    @property
    def tests(self):
        return self._tests

    @property
    def custom_workloads(self):
        return self._custom_workloads

    @property
    def memgraph_args(self):
        return self._memgraph_args

    @property
    def env(self):
        return self._env

    @property
    def coordinators(self):
        return self._coordinators

    @property
    def data_instances(self):
        return self._data_instances


# =============================================================================
# Test Execution
# =============================================================================


def parse_arguments() -> Args:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Run stress tests on Memgraph.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run all enabled workloads for a deployment
  ./continuous_integration --deployment docker_ha

  # Run a specific config file
  ./continuous_integration --config-file docker_ha/workloads/rag/vector_workload.yaml
        """,
    )

    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "--deployment", "-d", choices=DeploymentType.values(), help="Run all enabled workloads for a deployment type"
    )
    group.add_argument("--config-file", "-c", help="Path to a specific config YAML file")

    parser.add_argument(
        "--python",
        default=os.path.join(BASE_DIR, "tests", "ve3", "bin", "python3"),
        type=str,
        help="Path to Python interpreter",
    )

    return parser.parse_args()


def run_test(args: Args, config: Config, test: str, options: List[str], timeout: int) -> float:
    """Runs a test for Memgraph."""
    print(f"Running test '{test}'")

    binary = _find_test_binary(args, config, test)

    cmd = binary + ["--worker-count", str(THREADS)] + options
    start = time.time()
    ret_test = subprocess.run(cmd, cwd=SCRIPT_DIR, timeout=timeout * 60)

    if ret_test.returncode != 0:
        raise Exception(f"Test '{test}' binary returned non-zero ({ret_test.returncode})!")

    runtime = time.time() - start
    print(f"    Done after {runtime:.3f} seconds")
    return runtime


def _find_test_binary(args: Args, config: Config, test: str) -> List[str]:
    if test.endswith(".py"):
        logging = "DEBUG" if config.is_verbose else "WARNING"
        return [args.python, "-u", os.path.join(SCRIPT_DIR, test), "--logging", logging]

    if test.endswith(".cpp"):
        exe = os.path.join(BUILD_DIR, "tests", "stress", test[:-4])
        return [exe]

    raise Exception(f"Test '{test}' binary not supported!")


def run_workload_script(workload: Dict[str, Any], args: Args, timeout_sec: int) -> None:
    """Executes a custom Python script for a workload."""
    workload_script = workload["script"]
    script_args = workload.get("script_args", [])

    # Resolve script path (relative to stress directory or absolute)
    if not os.path.isabs(workload_script):
        workload_script = os.path.join(SCRIPT_DIR, workload_script)

    if not os.path.exists(workload_script):
        raise Exception(f"Workload script not found: {workload_script}")

    # Build command
    cmd = [args.python, "-u", workload_script]
    cmd.extend(script_args)

    print(f"Executing workload script: {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=SCRIPT_DIR, timeout=timeout_sec)

    if result.returncode != 0:
        raise Exception(f"Workload script returned non-zero ({result.returncode})!")


def run_custom_workloads(script_path: str, config: Config, args: Args) -> Dict[str, Any]:
    """Runs custom workloads on Memgraph using GQLAlchemy or custom scripts."""
    print("Running custom workloads...")
    runtimes = {}
    manage_deployment = script_path is not None

    for workload in config.custom_workloads:
        workload_name = workload["name"]
        print(f"Running workload '{workload_name}'")
        start = time.time()

        # Start Memgraph deployment (skip if no deployment script - cluster managed externally)
        if manage_deployment:
            try:
                start_deployment(script_path, config.memgraph_args + workload.get("memgraph_args", []), config.env)
            except Exception as ex:
                stop_deployment(script_path)
                return {"error": f"Failed to start deployment for workload '{workload_name}': {ex}"}
        else:
            print("No deployment script specified - assuming cluster is managed externally")

        try:
            # Run import queries
            print(f"Started import for workload '{workload_name}'")

            query_host = workload.get("querying", {}).get("host", "127.0.0.1")
            query_port = workload.get("querying", {}).get("port", 7687)

            memgraph = Memgraph(host=query_host, port=query_port)
            for query in workload.get("import", {}).get("queries", []):
                print(f"Executing query: {query}")
                memgraph.execute(query)
            print(f"Finished import for workload '{workload_name}'")

            timeout_sec = workload.get("timeout_min", 10) * 60

            # Check if workload uses a custom script or workers
            if "script" in workload:
                run_workload_script(workload, args, timeout_sec)
            else:
                # Execute workers in steps
                processes = []
                workers = workload.get("workers", [])
                steps = get_worker_steps(workers)

                for step in steps:
                    print(f"Running step {step}.")

                    for worker in [x for x in workers if x.get("step", 1) == step]:
                        # Override with workload global querying if not set on the worker
                        if "querying" not in worker.keys():
                            worker["querying"] = {"host": query_host, "port": query_port}
                        worker_object = get_worker_object(worker)

                        for i in range(worker.get("replicas", 1)):
                            process = multiprocessing.Process(target=worker_object.run)
                            processes.append((process, timeout_sec))
                            process.start()

                    # Monitor processes and enforce timeouts
                    for process, timeout in processes:
                        process.join(timeout)
                        if process.is_alive():
                            print(f"Worker process exceeded timeout of {timeout // 60} minutes. Terminating...")
                            process.terminate()
                            process.join()

            print(f"Finished workload '{workload_name}'")
            end = time.time()
            runtimes[workload_name] = end - start

        except Exception as ex:
            if manage_deployment:
                stop_deployment(script_path)
            return {"error": f"Failed to execute workload '{workload_name}': {ex}"}
        finally:
            if manage_deployment:
                stop_deployment(script_path)

    return runtimes


def run_config_file(args: Args, config_file: str) -> Dict[str, Any]:
    """Run tests from a single config file."""
    runtimes = {}

    config = Config(yaml.safe_load(open(config_file)))

    # Determine if we're managing deployment or using external cluster
    script_path = config.script
    manage_deployment = bool(script_path)

    if manage_deployment:
        script_path = os.path.join(SCRIPT_DIR, script_path)
        if not os.path.exists(script_path):
            return {"error": f"Memgraph deployment script not found: {script_path}!"}
    else:
        script_path = None
        print("No deployment script specified - assuming cluster is managed externally")

    # Run dataset tests
    for test in config.tests:
        test_name = test["name"]
        test_flags = test["test_args"]
        timeout = test["timeout_min"]
        memgraph_flags = config.memgraph_args + test.get("memgraph_args", [])

        if manage_deployment:
            try:
                start_deployment(script_path, memgraph_flags, config.env)
            except Exception as ex:
                return {"error": f"Failed to start deployment for test '{test_name}': {ex}"}

        try:
            runtime = run_test(args, config, test_name, test_flags, timeout)
            runtimes[os.path.splitext(test_name)[0]] = runtime
        except Exception as ex:
            if manage_deployment:
                stop_deployment(script_path)
            return {"error": f"Failed to execute test '{test_name}': {ex}"}
        finally:
            if manage_deployment:
                stop_deployment(script_path)

    # Run custom workloads
    workload_result = run_custom_workloads(script_path, config, args)
    if "error" in workload_result:
        return workload_result

    return runtimes | workload_result


def run_deployment(args: Args, deployment: str) -> Dict[str, Any]:
    """
    Run all enabled workloads for a deployment type.
    Reads from {deployment}/workloads.yaml.
    """
    print(f"\n{'='*60}")
    print(f"Running deployment: {deployment}")
    print(f"{'='*60}\n")

    workloads = load_workloads_registry(deployment)

    if not workloads:
        return {"error": f"No enabled workloads found for deployment '{deployment}'"}

    all_runtimes = {}

    for i, workload in enumerate(workloads, 1):
        config_path = workload["config_path"]
        relative_path = workload["relative_path"]

        print(f"\n[{i}/{len(workloads)}] Running: {relative_path}")
        print("-" * 60)

        result = run_config_file(args, config_path)

        if "error" in result:
            print(f"Error in workload: {result['error']}")
            return result

        # Prefix runtimes with workload name to avoid collisions
        workload_prefix = os.path.splitext(os.path.basename(relative_path))[0]
        for key, value in result.items():
            all_runtimes[f"{workload_prefix}.{key}"] = value

    return all_runtimes


def write_stats(runtimes: Dict[str, float]) -> None:
    """Writes stress test results to a file."""
    with open(MEASUREMENTS_FILE, "w") as f:
        f.write("\n".join(f"{key}.runtime {value}" for key, value in runtimes.items()))


# =============================================================================
# Main Entry Point
# =============================================================================


if __name__ == "__main__":
    args = parse_arguments()

    # Handle --deployment
    if args.deployment:
        result = run_deployment(args, args.deployment)
    # Handle --config-file
    elif args.config_file:
        config_file = args.config_file
        if not os.path.isabs(config_file):
            config_file = os.path.join(SCRIPT_DIR, config_file)
        result = run_config_file(args, config_file)
    # Default: run native_standalone with default config
    else:
        default_config = os.path.join(SCRIPT_DIR, "shared/templates/config_small.yaml")
        print(f"No deployment or config specified. Using default: {default_config}")
        result = run_config_file(args, default_config)

    if "error" in result:
        print(f"\nStress test failed: {result['error']}")
        sys.exit(1)

    write_stats(result)
    print("\nSuccessfully ran stress tests!")
